{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from itertools import combinations\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define the tickers for demonstration (excluding S&P 500 index)\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2024-06-01'\n",
    "\n",
    "# Download the data\n",
    "data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Download S&P 500 index data\n",
    "sp500 = yf.download('^GSPC', start=start_date, end=end_date)['Adj Close']\n",
    "sp500.dropna(inplace=True)\n",
    "\n",
    "# Calculate log returns\n",
    "log_returns = np.log(data).diff().dropna()\n",
    "sp500_log_returns = np.log(sp500).diff().dropna()\n",
    "\n",
    "# Normalize log returns\n",
    "normalized_log_returns = (log_returns - log_returns.mean()) / log_returns.std()\n",
    "\n",
    "# Ensure no NaNs or infinite values\n",
    "assert not normalized_log_returns.isnull().values.any(), \"Log returns data contains NaN values\"\n",
    "assert np.isfinite(normalized_log_returns.values).all(), \"Log returns data contains infinite values\"\n",
    "\n",
    "# Function to test cointegration for a pair using Johansen test\n",
    "def johansen_cointegration_test(pair, data):\n",
    "    log_ret_pair = data[list(pair)]\n",
    "    johansen_test = coint_johansen(log_ret_pair, det_order=0, k_ar_diff=1)\n",
    "    trace_statistic = johansen_test.lr1\n",
    "    critical_values = johansen_test.cvt[:, 1]  # 5% critical values\n",
    "    num_cointegrating_relations = sum(trace_statistic > critical_values)\n",
    "    cointegration_vectors = johansen_test.evec if num_cointegrating_relations > 0 else None\n",
    "    return num_cointegrating_relations, pair, cointegration_vectors\n",
    "\n",
    "# Function to test cointegration for a pair using Engle-Granger test\n",
    "def engle_granger_cointegration_test(pair, data):\n",
    "    log_ret_pair = data[list(pair)]\n",
    "    score, pvalue, _ = coint(log_ret_pair.iloc[:, 0], log_ret_pair.iloc[:, 1])\n",
    "    return pvalue, pair\n",
    "\n",
    "# Split the data into in-sample and out-of-sample\n",
    "split_date = '2020-07-01'\n",
    "in_sample_data = normalized_log_returns[:split_date]\n",
    "out_sample_data = normalized_log_returns[split_date:]\n",
    "\n",
    "# Get all possible pairs\n",
    "pairs = list(combinations(in_sample_data.columns, 2))\n",
    "\n",
    "# Test cointegration for all pairs using in-sample data\n",
    "johansen_results = [johansen_cointegration_test(pair, in_sample_data) for pair in pairs]\n",
    "engle_granger_results = [engle_granger_cointegration_test(pair, in_sample_data) for pair in pairs]\n",
    "\n",
    "# Sort and select the top 3 pairs based on Johansen test\n",
    "johansen_results.sort(reverse=True, key=lambda x: x[0])\n",
    "top_pairs_johansen = johansen_results[:3]\n",
    "\n",
    "# Sort and select the top 3 pairs based on Engle-Granger test\n",
    "engle_granger_results.sort(key=lambda x: x[0])\n",
    "top_pairs_engle_granger = engle_granger_results[:3]\n",
    "\n",
    "# Output the selected pairs for each method\n",
    "print(\"Top 3 pairs based on Johansen test:\")\n",
    "for i, pair in enumerate(top_pairs_johansen):\n",
    "    print(f\"Pair {i+1}: {pair[1]}\")\n",
    "\n",
    "print(\"\\nTop 3 pairs based on Engle-Granger test:\")\n",
    "for i, pair in enumerate(top_pairs_engle_granger):\n",
    "    print(f\"Pair {i+1}: {pair[1]}\")\n",
    "\n",
    "# Function to calculate beta relative to a benchmark\n",
    "def calculate_beta(stock_returns, benchmark_returns):\n",
    "    X = sm.add_constant(stock_returns)\n",
    "    model = sm.OLS(benchmark_returns, X).fit()\n",
    "    return model.params[1]  # Beta coefficient\n",
    "\n",
    "# Parameters for entry, exit, and stop-loss\n",
    "entry_threshold = 2\n",
    "exit_threshold = 0.5\n",
    "stop_loss_threshold = 3\n",
    "\n",
    "# Calculate the spreads, Z-scores, and positions for the top pairs\n",
    "def calculate_spreads_and_positions(top_pairs, in_sample_data, out_sample_data, benchmark_returns, method):\n",
    "    spreads = []\n",
    "    z_scores = []\n",
    "    positions_volatility = []\n",
    "    positions_beta = []\n",
    "    positions_dollar = []\n",
    "    for result in top_pairs:\n",
    "        if method == 'johansen':\n",
    "            cointegration_vector = result[2][:, 0]\n",
    "            spread = in_sample_data[result[1][0]] * cointegration_vector[0] - in_sample_data[result[1][1]] * cointegration_vector[1]\n",
    "            spread_out_sample = out_sample_data[result[1][0]] * cointegration_vector[0] - out_sample_data[result[1][1]] * cointegration_vector[1]\n",
    "        else:\n",
    "            beta, _ = np.polyfit(in_sample_data[result[1][0]], in_sample_data[result[1][1]], 1)\n",
    "            spread = in_sample_data[result[1][0]] - beta * in_sample_data[result[1][1]]\n",
    "            spread_out_sample = out_sample_data[result[1][0]] - beta * out_sample_data[result[1][1]]\n",
    "\n",
    "        spreads.append(spread)\n",
    "\n",
    "        spread_mean = spread_out_sample.rolling(window=252).mean()\n",
    "        spread_std = spread_out_sample.rolling(window=252).std()\n",
    "        z_score = (spread_out_sample - spread_mean) / spread_std\n",
    "        z_scores.append(z_score)\n",
    "\n",
    "        # Calculate position sizes for volatility, beta, and dollar neutrality\n",
    "        volatility_stock1 = in_sample_data[result[1][0]].std()\n",
    "        volatility_stock2 = in_sample_data[result[1][1]].std()\n",
    "        position_ratio_stock1_vol = 1 / volatility_stock1\n",
    "        position_ratio_stock2_vol = 1 / volatility_stock2\n",
    "        total_ratio_vol = position_ratio_stock1_vol + position_ratio_stock2_vol\n",
    "        position_ratio_stock1_vol /= total_ratio_vol\n",
    "        position_ratio_stock2_vol /= total_ratio_vol\n",
    "\n",
    "        beta_stock1 = calculate_beta(log_returns[result[1][0]], benchmark_returns)\n",
    "        beta_stock2 = calculate_beta(log_returns[result[1][1]], benchmark_returns)\n",
    "        position_ratio_stock1_beta = 1 / beta_stock1\n",
    "        position_ratio_stock2_beta = 1 / beta_stock2\n",
    "        total_ratio_beta = position_ratio_stock1_beta + position_ratio_stock2_beta\n",
    "        position_ratio_stock1_beta /= total_ratio_beta\n",
    "        position_ratio_stock2_beta /= total_ratio_beta\n",
    "\n",
    "        price_stock1 = data[result[1][0]].iloc[-1]  # Most recent price\n",
    "        price_stock2 = data[result[1][1]].iloc[-1]  # Most recent price\n",
    "        investment_amount = 10000  # Example investment amount per stock\n",
    "        shares_stock1 = investment_amount / price_stock1\n",
    "        shares_stock2 = investment_amount / price_stock2\n",
    "        total_shares = shares_stock1 + shares_stock2\n",
    "        position_ratio_stock1_dollar = shares_stock1 / total_shares\n",
    "        position_ratio_stock2_dollar = shares_stock2 / total_shares\n",
    "\n",
    "        def generate_positions(z_score, position_ratio_stock1, position_ratio_stock2):\n",
    "            positions = []\n",
    "            entry_dates = []\n",
    "            exit_dates = []\n",
    "            stop_loss_dates = []\n",
    "            initial_entry_z_score = None\n",
    "            for i in range(len(z_score)):\n",
    "                if positions and any(positions[-1]):\n",
    "                    if initial_entry_z_score is None:\n",
    "                        initial_entry_z_score = z_score[i]\n",
    "                    if abs(z_score[i] - initial_entry_z_score) >= stop_loss_threshold:\n",
    "                        positions.append([0, 0])\n",
    "                        initial_entry_z_score = None\n",
    "                        stop_loss_dates.append(z_score.index[i])\n",
    "                        continue\n",
    "                \n",
    "                if z_score[i] > entry_threshold:\n",
    "                    positions.append([-position_ratio_stock1, position_ratio_stock2])\n",
    "                    entry_dates.append(z_score.index[i])\n",
    "                    initial_entry_z_score = z_score[i]\n",
    "                elif z_score[i] < -entry_threshold:\n",
    "                    positions.append([position_ratio_stock1, -position_ratio_stock2])\n",
    "                    entry_dates.append(z_score.index[i])\n",
    "                    initial_entry_z_score = z_score[i]\n",
    "                elif abs(z_score[i]) < exit_threshold:\n",
    "                    positions.append([0, 0])\n",
    "                    exit_dates.append(z_score.index[i])\n",
    "                    initial_entry_z_score = None\n",
    "                else:\n",
    "                    positions.append(positions[-1] if positions else [0, 0])\n",
    "            return positions, entry_dates, exit_dates, stop_loss_dates\n",
    "\n",
    "        positions_volatility.append(generate_positions(z_score, position_ratio_stock1_vol, position_ratio_stock2_vol))\n",
    "        positions_beta.append(generate_positions(z_score, position_ratio_stock1_beta, position_ratio_stock2_beta))\n",
    "        positions_dollar.append(generate_positions(z_score, position_ratio_stock1_dollar, position_ratio_stock2_dollar))\n",
    "\n",
    "    return spreads, z_scores, positions_volatility, positions_beta, positions_dollar\n",
    "\n",
    "# Calculate spreads and positions for Johansen method\n",
    "spreads_johansen, z_scores_johansen, positions_volatility_johansen, positions_beta_johansen, positions_dollar_johansen = calculate_spreads_and_positions(\n",
    "    top_pairs_johansen, in_sample_data, out_sample_data, sp500_log_returns, method='johansen')\n",
    "\n",
    "# Calculate spreads and positions for Engle-Granger method\n",
    "spreads_engle_granger, z_scores_engle_granger, positions_volatility_engle_granger, positions_beta_engle_granger, positions_dollar_engle_granger = calculate_spreads_and_positions(\n",
    "    top_pairs_engle_granger, in_sample_data, out_sample_data, sp500_log_returns, method='engle_granger')\n",
    "\n",
    "# Convert positions to DataFrames and align indexes\n",
    "positions_volatility_johansen_dfs = [pd.DataFrame(pos[0], columns=[pair[1][0], pair[1][1]], index=z_score.index).reindex(out_sample_data.index).fillna(0) for pos, z_score, pair in zip(positions_volatility_johansen, z_scores_johansen, top_pairs_johansen)]\n",
    "positions_beta_johansen_dfs = [pd.DataFrame(pos[0], columns=[pair[1][0], pair[1][1]], index=z_score.index).reindex(out_sample_data.index).fillna(0) for pos, z_score, pair in zip(positions_beta_johansen, z_scores_johansen, top_pairs_johansen)]\n",
    "positions_dollar_johansen_dfs = [pd.DataFrame(pos[0], columns=[pair[1][0], pair[1][1]], index=z_score.index).reindex(out_sample_data.index).fillna(0) for pos, z_score, pair in zip(positions_dollar_johansen, z_scores_johansen, top_pairs_johansen)]\n",
    "\n",
    "positions_volatility_engle_granger_dfs = [pd.DataFrame(pos[0], columns=[pair[1][0], pair[1][1]], index=z_score.index).reindex(out_sample_data.index).fillna(0) for pos, z_score, pair in zip(positions_volatility_engle_granger, z_scores_engle_granger, top_pairs_engle_granger)]\n",
    "positions_beta_engle_granger_dfs = [pd.DataFrame(pos[0], columns=[pair[1][0], pair[1][1]], index=z_score.index).reindex(out_sample_data.index).fillna(0) for pos, z_score, pair in zip(positions_beta_engle_granger, z_scores_engle_granger, top_pairs_engle_granger)]\n",
    "positions_dollar_engle_granger_dfs = [pd.DataFrame(pos[0], columns=[pair[1][0], pair[1][1]], index=z_score.index).reindex(out_sample_data.index).fillna(0) for pos, z_score, pair in zip(positions_dollar_engle_granger, z_scores_engle_granger, top_pairs_engle_granger)]\n",
    "\n",
    "# Calculate daily returns of the selected pairs\n",
    "selected_pairs = [pair[1] for pair in top_pairs_johansen + top_pairs_engle_granger]\n",
    "returns_out_sample = log_returns.loc[split_date:][list(set([item for sublist in selected_pairs for item in sublist]))].dropna()\n",
    "\n",
    "# Calculate daily PnL for each method and each neutrality\n",
    "def calculate_daily_pnl(positions_dfs, returns_out_sample, selected_pairs):\n",
    "    daily_pnls = []\n",
    "    for pos_df, pair in zip(positions_dfs, selected_pairs):\n",
    "        daily_pnl = (pos_df.shift(1) * returns_out_sample[[pair[0], pair[1]]]).sum(axis=1)\n",
    "        daily_pnls.append(daily_pnl)\n",
    "    return daily_pnls\n",
    "\n",
    "daily_pnls_volatility_johansen = calculate_daily_pnl(positions_volatility_johansen_dfs, returns_out_sample, [pair[1] for pair in top_pairs_johansen])\n",
    "daily_pnls_beta_johansen = calculate_daily_pnl(positions_beta_johansen_dfs, returns_out_sample, [pair[1] for pair in top_pairs_johansen])\n",
    "daily_pnls_dollar_johansen = calculate_daily_pnl(positions_dollar_johansen_dfs, returns_out_sample, [pair[1] for pair in top_pairs_johansen])\n",
    "\n",
    "daily_pnls_volatility_engle_granger = calculate_daily_pnl(positions_volatility_engle_granger_dfs, returns_out_sample, [pair[1] for pair in top_pairs_engle_granger])\n",
    "daily_pnls_beta_engle_granger = calculate_daily_pnl(positions_beta_engle_granger_dfs, returns_out_sample, [pair[1] for pair in top_pairs_engle_granger])\n",
    "daily_pnls_dollar_engle_granger = calculate_daily_pnl(positions_dollar_engle_granger_dfs, returns_out_sample, [pair[1] for pair in top_pairs_engle_granger])\n",
    "\n",
    "# Calculate cumulative PnL for each method and each neutrality\n",
    "def calculate_cumulative_pnl(daily_pnls):\n",
    "    return [daily_pnl.cumsum() for daily_pnl in daily_pnls]\n",
    "\n",
    "cumulative_pnls_volatility_johansen = calculate_cumulative_pnl(daily_pnls_volatility_johansen)\n",
    "cumulative_pnls_beta_johansen = calculate_cumulative_pnl(daily_pnls_beta_johansen)\n",
    "cumulative_pnls_dollar_johansen = calculate_cumulative_pnl(daily_pnls_dollar_johansen)\n",
    "\n",
    "cumulative_pnls_volatility_engle_granger = calculate_cumulative_pnl(daily_pnls_volatility_engle_granger)\n",
    "cumulative_pnls_beta_engle_granger = calculate_cumulative_pnl(daily_pnls_beta_engle_granger)\n",
    "cumulative_pnls_dollar_engle_granger = calculate_cumulative_pnl(daily_pnls_dollar_engle_granger)\n",
    "\n",
    "# Calculate equal weight portfolio PnL\n",
    "total_daily_pnl_volatility = sum(daily_pnls_volatility_johansen + daily_pnls_volatility_engle_granger) / len(daily_pnls_volatility_johansen + daily_pnls_volatility_engle_granger)\n",
    "total_daily_pnl_beta = sum(daily_pnls_beta_johansen + daily_pnls_beta_engle_granger) / len(daily_pnls_beta_johansen + daily_pnls_beta_engle_granger)\n",
    "total_daily_pnl_dollar = sum(daily_pnls_dollar_johansen + daily_pnls_dollar_engle_granger) / len(daily_pnls_dollar_johansen + daily_pnls_dollar_engle_granger)\n",
    "\n",
    "cumulative_pnl_portfolio_volatility = total_daily_pnl_volatility.cumsum()\n",
    "cumulative_pnl_portfolio_beta = total_daily_pnl_beta.cumsum()\n",
    "cumulative_pnl_portfolio_dollar = total_daily_pnl_dollar.cumsum()\n",
    "\n",
    "# Plot cumulative PnL for each method and the portfolio\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, cumulative_pnl in enumerate(cumulative_pnls_volatility_johansen):\n",
    "    plt.plot(cumulative_pnl, label=f'Volatility Johansen Pair {i+1}')\n",
    "for i, cumulative_pnl in enumerate(cumulative_pnls_volatility_engle_granger):\n",
    "    plt.plot(cumulative_pnl, label=f'Volatility Engle-Granger Pair {i+1}')\n",
    "plt.plot(cumulative_pnl_portfolio_volatility, label='Equal Weight Portfolio (Volatility)', linewidth=2, color='black')\n",
    "plt.title('Cumulative PnL of the Cointegration-Based Pairs Trading Strategy (Out-of-Sample)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, cumulative_pnl in enumerate(cumulative_pnls_beta_johansen):\n",
    "    plt.plot(cumulative_pnl, label=f'Beta Johansen Pair {i+1}')\n",
    "for i, cumulative_pnl in enumerate(cumulative_pnls_beta_engle_granger):\n",
    "    plt.plot(cumulative_pnl, label=f'Beta Engle-Granger Pair {i+1}')\n",
    "plt.plot(cumulative_pnl_portfolio_beta, label='Equal Weight Portfolio (Beta)', linewidth=2, color='black')\n",
    "plt.title('Cumulative PnL of the Cointegration-Based Pairs Trading Strategy (Out-of-Sample)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, cumulative_pnl in enumerate(cumulative_pnls_dollar_johansen):\n",
    "    plt.plot(cumulative_pnl, label=f'Dollar Johansen Pair {i+1}')\n",
    "for i, cumulative_pnl in enumerate(cumulative_pnls_dollar_engle_granger):\n",
    "    plt.plot(cumulative_pnl, label=f'Dollar Engle-Granger Pair {i+1}')\n",
    "plt.plot(cumulative_pnl_portfolio_dollar, label='Equal Weight Portfolio (Dollar)', linewidth=2, color='black')\n",
    "plt.title('Cumulative PnL of the Cointegration-Based Pairs Trading Strategy (Out-of-Sample)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print cumulative PnL for each method and the portfolio\n",
    "def print_cumulative_pnls(cumulative_pnls, method):\n",
    "    for i, cumulative_pnl in enumerate(cumulative_pnls):\n",
    "        print(f\"Cumulative PnL ({method} Pair {i+1}): {cumulative_pnl.iloc[-1]:.2f}\")\n",
    "\n",
    "print_cumulative_pnls(cumulative_pnls_volatility_johansen, 'Volatility Johansen')\n",
    "print_cumulative_pnls(cumulative_pnls_volatility_engle_granger, 'Volatility Engle-Granger')\n",
    "print(f\"Cumulative PnL (Equal Weight Portfolio - Volatility): {cumulative_pnl_portfolio_volatility.iloc[-1]:.2f}\")\n",
    "\n",
    "print_cumulative_pnls(cumulative_pnls_beta_johansen, 'Beta Johansen')\n",
    "print_cumulative_pnls(cumulative_pnls_beta_engle_granger, 'Beta Engle-Granger')\n",
    "print(f\"Cumulative PnL (Equal Weight Portfolio - Beta): {cumulative_pnl_portfolio_beta.iloc[-1]:.2f}\")\n",
    "\n",
    "print_cumulative_pnls(cumulative_pnls_dollar_johansen, 'Dollar Johansen')\n",
    "print_cumulative_pnls(cumulative_pnls_dollar_engle_granger, 'Dollar Engle-Granger')\n",
    "print(f\"Cumulative PnL (Equal Weight Portfolio - Dollar): {cumulative_pnl_portfolio_dollar.iloc[-1]:.2f}\")\n",
    "\n",
    "# Print strategy performance metrics for each method\n",
    "def print_performance_metrics(cumulative_pnl, daily_pnl):\n",
    "    total_return = cumulative_pnl.iloc[-1]\n",
    "    annualized_return = (1 + total_return) ** (252 / len(cumulative_pnl)) - 1  # Assuming 252 trading days in a year\n",
    "    annualized_volatility = daily_pnl.std() * np.sqrt(252)\n",
    "    sharpe_ratio = annualized_return / annualized_volatility\n",
    "\n",
    "    print(f\"Total Return: {total_return * 100:.2f}%\")\n",
    "    print(f\"Annualized Return: {annualized_return * 100:.2f}%\")\n",
    "    print(f\"Annualized Volatility: {annualized_volatility * 100:.2f}%\")\n",
    "    print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "\n",
    "for i, (cumulative_pnl, daily_pnl) in enumerate(zip(cumulative_pnls_volatility_johansen, daily_pnls_volatility_johansen)):\n",
    "    print(f\"Performance Metrics (Volatility Johansen Pair {i+1}):\")\n",
    "    print_performance_metrics(cumulative_pnl, daily_pnl)\n",
    "\n",
    "for i, (cumulative_pnl, daily_pnl) in enumerate(zip(cumulative_pnls_volatility_engle_granger, daily_pnls_volatility_engle_granger)):\n",
    "    print(f\"Performance Metrics (Volatility Engle-Granger Pair {i+1}):\")\n",
    "    print_performance_metrics(cumulative_pnl, daily_pnl)\n",
    "\n",
    "print(\"Performance Metrics (Equal Weight Portfolio - Volatility):\")\n",
    "print_performance_metrics(cumulative_pnl_portfolio_volatility, total_daily_pnl_volatility)\n",
    "\n",
    "for i, (cumulative_pnl, daily_pnl) in enumerate(zip(cumulative_pnls_beta_johansen, daily_pnls_beta_johansen)):\n",
    "    print(f\"Performance Metrics (Beta Johansen Pair {i+1}):\")\n",
    "    print_performance_metrics(cumulative_pnl, daily_pnl)\n",
    "\n",
    "for i, (cumulative_pnl, daily_pnl) in enumerate(zip(cumulative_pnls_beta_engle_granger, daily_pnls_beta_engle_granger)):\n",
    "    print(f\"Performance Metrics (Beta Engle-Granger Pair {i+1}):\")\n",
    "    print_performance_metrics(cumulative_pnl, daily_pnl)\n",
    "\n",
    "print(\"Performance Metrics (Equal Weight Portfolio - Beta):\")\n",
    "print_performance_metrics(cumulative_pnl_portfolio_beta, total_daily_pnl_beta)\n",
    "\n",
    "for i, (cumulative_pnl, daily_pnl) in enumerate(zip(cumulative_pnls_dollar_johansen, daily_pnls_dollar_johansen)):\n",
    "    print(f\"Performance Metrics (Dollar Johansen Pair {i+1}):\")\n",
    "    print_performance_metrics(cumulative_pnl, daily_pnl)\n",
    "\n",
    "for i, (cumulative_pnl, daily_pnl) in enumerate(zip(cumulative_pnls_dollar_engle_granger, daily_pnls_dollar_engle_granger)):\n",
    "    print(f\"Performance Metrics (Dollar Engle-Granger Pair {i+1}):\")\n",
    "    print_performance_metrics(cumulative_pnl, daily_pnl)\n",
    "\n",
    "print(\"Performance Metrics (Equal Weight Portfolio - Dollar):\")\n",
    "print_performance_metrics(cumulative_pnl_portfolio_dollar, total_daily_pnl_dollar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58652c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from itertools import combinations\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define the tickers for demonstration (excluding S&P 500 index)\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2024-06-01'\n",
    "\n",
    "# Download the data\n",
    "data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Download S&P 500 index data\n",
    "sp500 = yf.download('^GSPC', start=start_date, end=end_date)['Adj Close']\n",
    "sp500.dropna(inplace=True)\n",
    "\n",
    "# Calculate log returns\n",
    "log_returns = np.log(data).diff().dropna()\n",
    "sp500_log_returns = np.log(sp500).diff().dropna()\n",
    "\n",
    "# Normalize log returns\n",
    "normalized_log_returns = (log_returns - log_returns.mean()) / log_returns.std()\n",
    "\n",
    "# Function to test cointegration for a pair using Johansen test\n",
    "def johansen_cointegration_test(pair, data):\n",
    "    log_ret_pair = data[list(pair)]\n",
    "    johansen_test = coint_johansen(log_ret_pair, det_order=0, k_ar_diff=1)\n",
    "    trace_statistic = johansen_test.lr1\n",
    "    critical_values = johansen_test.cvt[:, 1]  # 5% critical values\n",
    "    num_cointegrating_relations = sum(trace_statistic > critical_values)\n",
    "    cointegration_vectors = johansen_test.evec if num_cointegrating_relations > 0 else None\n",
    "    return num_cointegrating_relations, pair, cointegration_vectors\n",
    "\n",
    "# Function to test cointegration for a pair using Engle-Granger test\n",
    "def engle_granger_cointegration_test(pair, data):\n",
    "    log_ret_pair = data[list(pair)]\n",
    "    score, pvalue, _ = coint(log_ret_pair.iloc[:, 0], log_ret_pair.iloc[:, 1])\n",
    "    return pvalue, pair\n",
    "\n",
    "# Split the data into in-sample and out-of-sample\n",
    "split_date = '2020-07-01'\n",
    "in_sample_data = normalized_log_returns[:split_date]\n",
    "out_sample_data = normalized_log_returns[split_date:]\n",
    "\n",
    "# Get all possible pairs\n",
    "pairs = list(combinations(in_sample_data.columns, 2))\n",
    "\n",
    "# Test cointegration for all pairs using in-sample data\n",
    "johansen_results = [johansen_cointegration_test(pair, in_sample_data) for pair in pairs]\n",
    "engle_granger_results = [engle_granger_cointegration_test(pair, in_sample_data) for pair in pairs]\n",
    "\n",
    "# Combine the pairs and remove duplicates\n",
    "pairs_johansen = [result[1] for result in johansen_results if result[0] > 0]  # Only keep pairs with at least 1 cointegrating relation\n",
    "pairs_engle_granger = [result[1] for result in engle_granger_results if result[0] < 0.05]  # Only keep pairs with p-value < 0.05\n",
    "combined_pairs = list(set(pairs_johansen + pairs_engle_granger))\n",
    "\n",
    "# Function to calculate beta relative to a benchmark\n",
    "def calculate_beta(stock_returns, benchmark_returns):\n",
    "    X = sm.add_constant(stock_returns)\n",
    "    model = sm.OLS(benchmark_returns, X).fit()\n",
    "    return model.params[1]  # Beta coefficient\n",
    "\n",
    "# Parameters for entry, exit, and stop-loss\n",
    "entry_threshold = 2\n",
    "exit_threshold = 0.5\n",
    "stop_loss_threshold = 3\n",
    "\n",
    "# Calculate the spreads, Z-scores, and positions for the selected pairs\n",
    "def calculate_spreads_and_positions(pair, in_sample_data, out_sample_data):\n",
    "    spread = in_sample_data[pair[0]] - in_sample_data[pair[1]]\n",
    "    spread_out_sample = out_sample_data[pair[0]] - out_sample_data[pair[1]]\n",
    "    spread_mean = spread_out_sample.rolling(window=252).mean()\n",
    "    spread_std = spread_out_sample.rolling(window=252).std()\n",
    "    z_score = (spread_out_sample - spread_mean) / spread_std\n",
    "\n",
    "    positions = []\n",
    "    for i in range(len(z_score)):\n",
    "        if z_score[i] > entry_threshold:\n",
    "            positions.append([-1, 1])  # Short first, long second\n",
    "        elif z_score[i] < -entry_threshold:\n",
    "            positions.append([1, -1])  # Long first, short second\n",
    "        elif abs(z_score[i]) < exit_threshold:\n",
    "            positions.append([0, 0])  # Exit positions\n",
    "        else:\n",
    "            positions.append(positions[-1] if positions else [0, 0])\n",
    "    \n",
    "    positions_df = pd.DataFrame(positions, index=z_score.index, columns=[pair[0], pair[1]])\n",
    "    return spread, z_score, positions_df\n",
    "\n",
    "# Calculate Sharpe ratio\n",
    "def calculate_sharpe_ratio(daily_pnl):\n",
    "    total_return = daily_pnl.sum()\n",
    "    annualized_return = total_return * (252 / len(daily_pnl))  # Assuming 252 trading days in a year\n",
    "    annualized_volatility = daily_pnl.std() * np.sqrt(252)\n",
    "    sharpe_ratio = annualized_return / annualized_volatility\n",
    "    return sharpe_ratio\n",
    "\n",
    "# Evaluate all combined pairs and select top 3 by Sharpe ratio\n",
    "sharpe_ratios = []\n",
    "daily_pnls = []\n",
    "for pair in combined_pairs:\n",
    "    spread, z_score, positions_df = calculate_spreads_and_positions(pair, in_sample_data, out_sample_data)\n",
    "    daily_pnl = (positions_df.shift(1) * log_returns.loc[split_date:][[pair[0], pair[1]]]).sum(axis=1)\n",
    "    sharpe_ratio = calculate_sharpe_ratio(daily_pnl)\n",
    "    sharpe_ratios.append((sharpe_ratio, pair, daily_pnl))\n",
    "    daily_pnls.append(daily_pnl)\n",
    "\n",
    "# Sort by Sharpe ratio and select top 3 pairs\n",
    "sharpe_ratios.sort(reverse=True, key=lambda x: x[0])\n",
    "top_3_pairs = sharpe_ratios[:3]\n",
    "\n",
    "# Plot individual performances\n",
    "plt.figure(figsize=(12, 8))\n",
    "for sharpe, pair, daily_pnl in top_3_pairs:\n",
    "    cumulative_pnl = daily_pnl.cumsum()\n",
    "    plt.plot(cumulative_pnl, label=f'{pair} (Sharpe: {sharpe:.2f})')\n",
    "plt.title('Cumulative PnL of Top 3 Pairs by Sharpe Ratio (Out-of-Sample)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Combine top 3 pairs into a single portfolio\n",
    "portfolio_daily_pnl = sum([pnl for _, _, pnl in top_3_pairs]) / 3\n",
    "portfolio_cumulative_pnl = portfolio_daily_pnl.cumsum()\n",
    "\n",
    "# Plot portfolio performance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(portfolio_cumulative_pnl, label='Portfolio (Equal Weight of Top 3 Pairs)')\n",
    "plt.title('Cumulative PnL of the Portfolio (Top 3 Pairs by Sharpe Ratio)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print Sharpe ratios of top 3 pairs and portfolio\n",
    "print(\"Top 3 Pairs by Sharpe Ratio:\")\n",
    "for sharpe, pair, _ in top_3_pairs:\n",
    "    print(f\"Pair: {pair}, Sharpe Ratio: {sharpe:.2f}\")\n",
    "\n",
    "portfolio_sharpe_ratio = calculate_sharpe_ratio(portfolio_daily_pnl)\n",
    "print(f\"\\nPortfolio Sharpe Ratio: {portfolio_sharpe_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ab68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot spreads for the top 3 pairs\n",
    "plt.figure(figsize=(12, 8))\n",
    "for _, pair, _ in top_3_pairs:\n",
    "    spread, z_score, _ = calculate_spreads_and_positions(pair, in_sample_data, out_sample_data)\n",
    "    plt.plot(spread, label=f'Spread: {pair}')\n",
    "plt.title('Spreads of Top 3 Pairs by Sharpe Ratio')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc367069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print performance metrics for Johansen and Engle-Granger pairs\n",
    "print(\"Performance Metrics for Johansen Test Pairs:\")\n",
    "for sharpe, pair, pnl in sharpe_ratios[:3]:\n",
    "    total_return = pnl.sum()\n",
    "    annualized_return = total_return * (252 / len(pnl))\n",
    "    annualized_volatility = pnl.std() * np.sqrt(252)\n",
    "    sharpe_ratio = annualized_return / annualized_volatility\n",
    "    print(f\"Pair: {pair}, Total Return: {total_return:.2f}, Annualized Return: {annualized_return:.2f}, Annualized Volatility: {annualized_volatility:.2f}, Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "\n",
    "print(\"\\nPerformance Metrics for Engle-Granger Test Pairs:\")\n",
    "for sharpe, pair, pnl in sharpe_ratios[3:]:\n",
    "    total_return = pnl.sum()\n",
    "    annualized_return = total_return * (252 / len(pnl))\n",
    "    annualized_volatility = pnl.std() * np.sqrt(252)\n",
    "    sharpe_ratio = annualized_return / annualized_volatility\n",
    "    print(f\"Pair: {pair}, Total Return: {total_return:.2f}, Annualized Return: {annualized_return:.2f}, Annualized Volatility: {annualized_volatility:.2f}, Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "\n",
    "\n",
    "print(\"\\nPerformance Metrics for Portfolio (Top 3 Pairs):\")\n",
    "total_return = portfolio_daily_pnl.sum()\n",
    "annualized_return = total_return * (252 / len(portfolio_daily_pnl))\n",
    "annualized_volatility = portfolio_daily_pnl.std() * np.sqrt(252)\n",
    "sharpe_ratio = annualized_return / annualized_volatility\n",
    "print(f\"Total Return: {total_return:.2f}, Annualized Return: {annualized_return:.2f}, Annualized Volatility: {annualized_volatility:.2f}, Sharpe Ratio: {sharpe_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2f0e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transaction cost per trade as a percentage of the traded value\n",
    "transaction_cost_rate = 0.001  # 0.1% per trade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the spreads, Z-scores, and positions for the selected pairs, including transaction costs\n",
    "def calculate_spreads_and_positions(pair, in_sample_data, out_sample_data):\n",
    "    spread = in_sample_data[pair[0]] - in_sample_data[pair[1]]\n",
    "    spread_out_sample = out_sample_data[pair[0]] - out_sample_data[pair[1]]\n",
    "    spread_mean = spread_out_sample.rolling(window=252).mean()\n",
    "    spread_std = spread_out_sample.rolling(window=252).std()\n",
    "    z_score = (spread_out_sample - spread_mean) / spread_std\n",
    "\n",
    "    positions = []\n",
    "    for i in range(len(z_score)):\n",
    "        if z_score[i] > entry_threshold:\n",
    "            positions.append([-1, 1])  # Short first, long second\n",
    "        elif z_score[i] < -entry_threshold:\n",
    "            positions.append([1, -1])  # Long first, short second\n",
    "        elif abs(z_score[i]) < exit_threshold:\n",
    "            positions.append([0, 0])  # Exit positions\n",
    "        else:\n",
    "            positions.append(positions[-1] if positions else [0, 0])\n",
    "    \n",
    "    positions_df = pd.DataFrame(positions, index=z_score.index, columns=[pair[0], pair[1]])\n",
    "    return spread, z_score, positions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba61ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from itertools import combinations\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define the tickers for demonstration (excluding S&P 500 index)\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']\n",
    "start_date = '2013-01-01'\n",
    "end_date = '2024-06-01'\n",
    "\n",
    "# Download the data\n",
    "data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Download S&P 500 index data\n",
    "sp500 = yf.download('^GSPC', start=start_date, end=end_date)['Adj Close']\n",
    "sp500.dropna(inplace=True)\n",
    "\n",
    "# Calculate log returns\n",
    "log_returns = np.log(data).diff().dropna()\n",
    "sp500_log_returns = np.log(sp500).diff().dropna()\n",
    "\n",
    "# Normalize log returns\n",
    "normalized_log_returns = (log_returns - log_returns.mean()) / log_returns.std()\n",
    "\n",
    "# Function to test cointegration for a pair using Johansen test\n",
    "def johansen_cointegration_test(pair, data):\n",
    "    log_ret_pair = data[list(pair)]\n",
    "    johansen_test = coint_johansen(log_ret_pair, det_order=0, k_ar_diff=1)\n",
    "    trace_statistic = johansen_test.lr1\n",
    "    critical_values = johansen_test.cvt[:, 1]  # 5% critical values\n",
    "    num_cointegrating_relations = sum(trace_statistic > critical_values)\n",
    "    cointegration_vectors = johansen_test.evec if num_cointegrating_relations > 0 else None\n",
    "    return num_cointegrating_relations, pair, cointegration_vectors\n",
    "\n",
    "# Function to test cointegration for a pair using Engle-Granger test\n",
    "def engle_granger_cointegration_test(pair, data):\n",
    "    log_ret_pair = data[list(pair)]\n",
    "    score, pvalue, _ = coint(log_ret_pair.iloc[:, 0], log_ret_pair.iloc[:, 1])\n",
    "    return pvalue, pair\n",
    "\n",
    "# Split the data into in-sample and out-of-sample\n",
    "split_date = '2020-07-01'\n",
    "in_sample_data = normalized_log_returns[:split_date]\n",
    "out_sample_data = normalized_log_returns[split_date:]\n",
    "\n",
    "# Get all possible pairs\n",
    "pairs = list(combinations(in_sample_data.columns, 2))\n",
    "\n",
    "# Test cointegration for all pairs using in-sample data\n",
    "johansen_results = [johansen_cointegration_test(pair, in_sample_data) for pair in pairs]\n",
    "engle_granger_results = [engle_granger_cointegration_test(pair, in_sample_data) for pair in pairs]\n",
    "\n",
    "# Combine the pairs and remove duplicates\n",
    "pairs_johansen = [result[1] for result in johansen_results if result[0] > 0]  # Only keep pairs with at least 1 cointegrating relation\n",
    "pairs_engle_granger = [result[1] for result in engle_granger_results if result[0] < 0.05]  # Only keep pairs with p-value < 0.05\n",
    "combined_pairs = list(set(pairs_johansen + pairs_engle_granger))\n",
    "\n",
    "# Define the transaction cost per trade as a percentage of the traded value\n",
    "transaction_cost_rate = 0.001  # 0.1% per trade\n",
    "\n",
    "# Parameters for entry, exit, and stop-loss\n",
    "entry_threshold = 2\n",
    "exit_threshold = 0.5\n",
    "stop_loss_threshold = 3\n",
    "\n",
    "# Calculate the spreads, Z-scores, and positions for the selected pairs, including transaction costs\n",
    "def calculate_spreads_and_positions(pair, in_sample_data, out_sample_data):\n",
    "    spread = in_sample_data[pair[0]] - in_sample_data[pair[1]]\n",
    "    spread_out_sample = out_sample_data[pair[0]] - out_sample_data[pair[1]]\n",
    "    spread_mean = spread_out_sample.rolling(window=252).mean()\n",
    "    spread_std = spread_out_sample.rolling(window=252).std()\n",
    "    z_score = (spread_out_sample - spread_mean) / spread_std\n",
    "\n",
    "    positions = []\n",
    "    for i in range(len(z_score)):\n",
    "        if z_score[i] > entry_threshold:\n",
    "            positions.append([-1, 1])  # Short first, long second\n",
    "        elif z_score[i] < -entry_threshold:\n",
    "            positions.append([1, -1])  # Long first, short second\n",
    "        elif abs(z_score[i]) < exit_threshold:\n",
    "            positions.append([0, 0])  # Exit positions\n",
    "        else:\n",
    "            positions.append(positions[-1] if positions else [0, 0])\n",
    "    \n",
    "    positions_df = pd.DataFrame(positions, index=z_score.index, columns=[pair[0], pair[1]])\n",
    "    return spread, z_score, positions_df\n",
    "\n",
    "# Calculate Sharpe ratio, including transaction costs\n",
    "def calculate_sharpe_ratio(daily_pnl):\n",
    "    total_return = daily_pnl.sum()\n",
    "    annualized_return = total_return * (252 / len(daily_pnl))  # Assuming 252 trading days in a year\n",
    "    annualized_volatility = daily_pnl.std() * np.sqrt(252)\n",
    "    sharpe_ratio = annualized_return / annualized_volatility\n",
    "    return sharpe_ratio\n",
    "\n",
    "# Evaluate all combined pairs and select top 3 by Sharpe ratio, including transaction costs\n",
    "sharpe_ratios = []\n",
    "daily_pnls = []\n",
    "for pair in combined_pairs:\n",
    "    spread, z_score, positions_df = calculate_spreads_and_positions(pair, in_sample_data, out_sample_data)\n",
    "    daily_pnl = (positions_df.shift(1) * log_returns.loc[split_date:][[pair[0], pair[1]]]).sum(axis=1)\n",
    "    \n",
    "    # Calculate transaction costs\n",
    "    transaction_costs = transaction_cost_rate * positions_df.diff().abs().sum(axis=1)\n",
    "    daily_pnl -= transaction_costs\n",
    "    \n",
    "    sharpe_ratio = calculate_sharpe_ratio(daily_pnl)\n",
    "    sharpe_ratios.append((sharpe_ratio, pair, daily_pnl))\n",
    "    daily_pnls.append(daily_pnl)\n",
    "\n",
    "# Sort by Sharpe ratio and select top 3 pairs\n",
    "sharpe_ratios.sort(reverse=True, key=lambda x: x[0])\n",
    "top_3_pairs = sharpe_ratios[:3]\n",
    "\n",
    "# Combine top 3 pairs into a single portfolio\n",
    "portfolio_daily_pnl = sum([pnl for _, _, pnl in top_3_pairs]) / 3\n",
    "portfolio_cumulative_pnl = portfolio_daily_pnl.cumsum()\n",
    "\n",
    "# Plot portfolio performance with transaction costs\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(portfolio_cumulative_pnl, label='Portfolio (Equal Weight of Top 3 Pairs)')\n",
    "plt.title('Cumulative PnL of the Portfolio (Top 3 Pairs by Sharpe Ratio)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7854eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
